{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6u5mix4tiSiQnHaSroHFE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kfirs127/Ambiguity-Classifier-in-Open-Domain-Questions/blob/main/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "M1yZIpTIjJkz"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers torch scikit-learn pandas tqdm\n",
        "\n",
        "import json\n",
        "import glob\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.metrics import make_scorer, hamming_loss, f1_score, jaccard_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE = '/content/drive/Shared drives/Ambiguity Classifier in Open-Domain Questions'"
      ],
      "metadata": {
        "id": "6FIagouojnob"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDJwT58FjnrV",
        "outputId": "c981f4b8-150c-47fe-b142-fb5f52cbc28e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_cols = [\n",
        "    \"Event references\",\n",
        "    \"Properties\",\n",
        "    \"Entity references\",\n",
        "    \"Answer types\",\n",
        "    \"Time dependency\",\n",
        "    \"Multiple sub-questions\"\n",
        "]"
      ],
      "metadata": {
        "id": "RjsAX5g5wpdQ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_dirs = [os.path.join(BASE, d) for d in os.listdir(BASE) if d.lower().startswith('split_part')]\n",
        "sheet_ids = {\n",
        "    'split_part_1 - roi': '1KfLv3mogWEQtc-ucVkqWsQGtrklS1Wd-Rk2pS4avigc',\n",
        "    'split_part_2 - kfir': '10sGw8TXTytUgpdKqi9Myoyiof1PiU8wIsFcEnQU0SZc',\n",
        "    'split_part_3 - bar': '1suxKtlo4rEzVAPHHJBvpj7IlKTu35JkYyUNJ8Y3ha6k',\n",
        "    'split_part_4 - amit': '1DOSNgWgEVybI1DRyCu0Tv0a6C12ihJFiUit3wlCdLbs',\n",
        "}\n",
        "sheet_name = 'Labeling'\n"
      ],
      "metadata": {
        "id": "otDwGGkhoY9-"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def id_truncate(x):\n",
        "    x = str(x).strip()\n",
        "    if x.startswith('-'):\n",
        "        return x[1:]\n",
        "    return x\n",
        "\n",
        "\n",
        "all_data = []\n",
        "\n",
        "for split_dir in split_dirs:\n",
        "    folder_name = os.path.basename(split_dir)\n",
        "    SHEET_ID = sheet_ids[folder_name.lower()]\n",
        "    LABEL_URL = f'https://docs.google.com/spreadsheets/d/{SHEET_ID}/gviz/tq?tqx=out:csv&sheet={sheet_name}'\n",
        "    df_labels = pd.read_csv(LABEL_URL, dtype=str)\n",
        "    # Use plain string for IDs, remove scientific, strip\n",
        "    df_labels['Id'] = df_labels['Id'].astype(str).str.strip()\n",
        "\n",
        "    json_files = glob.glob(os.path.join(split_dir, '*.json'))\n",
        "    assert len(json_files) == 1, f\"Expected exactly 1 JSON in {split_dir}, found {len(json_files)}\"\n",
        "    with open(json_files[0], \"r\", encoding=\"utf-8\") as f:\n",
        "        questions = json.load(f)\n",
        "    df_questions = pd.DataFrame([\n",
        "        {\"Id\": str(q[\"id\"]).strip(), \"Question\": q[\"question\"]}\n",
        "        for q in questions\n",
        "    ])\n",
        "    df_questions['Id'] = df_questions['Id'].astype(str).str.strip()\n",
        "\n",
        "    # Create truncated key columns for merging\n",
        "    df_labels['Id_trunc'] = df_labels['Id'].apply(lambda x: id_truncate(x))\n",
        "    df_questions['Id_trunc'] = df_questions['Id'].apply(lambda x: id_truncate(x))\n",
        "\n",
        "    # Merge on truncated IDs\n",
        "    df = df_labels.merge(df_questions, on=\"Id_trunc\", suffixes=('_label', '_question'))\n",
        "    # Use the \"full\" ID from the questions side (to keep one good column)\n",
        "    df['Id'] = df['Id_question']\n",
        "    # Also keep only unique merged records by \"Id\"\n",
        "    df = df.drop_duplicates('Id')\n",
        "\n",
        "    # Keep only relevant columns\n",
        "    final_cols = ['Id', 'Question'] + [\n",
        "        \"Event references\",\n",
        "        \"Properties\",\n",
        "        \"Entity references\",\n",
        "        \"Answer types\",\n",
        "        \"Time dependency\",\n",
        "        \"Multiple sub-questions\"\n",
        "    ]\n",
        "    # If your Google Sheet has extra columns (e.g., 'Unnamed'), select only existing ones\n",
        "    final_cols_existing = [col for col in final_cols if col in df.columns]\n",
        "    df = df[final_cols_existing]\n",
        "\n",
        "    print(f\"[{folder_name}] After unsafe merge: {df.shape}\")\n",
        "\n",
        "    all_data.append(df)\n",
        "\n",
        "df_final = pd.concat(all_data, ignore_index=True)\n",
        "print(\"Total combined shape:\", df_final.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrBbMUfZjnuM",
        "outputId": "32ee8cc8-a394-4fcf-d468-8a49fe50425c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[split_part_4 - Amit] After unsafe merge: (99, 8)\n",
            "[split_part_3 - Bar] After unsafe merge: (200, 8)\n",
            "[split_part_2 - Kfir] After unsafe merge: (194, 8)\n",
            "[split_part_1 - Roi] After unsafe merge: (199, 8)\n",
            "Total combined shape: (692, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
        "bert.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_Xg12O6kxZF",
        "outputId": "88f9be0d-3c5d-4e71-d2d0-09f74824ac4b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSdpaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def get_bert_embeddings(texts, batch_size=1024):\n",
        "    all_embeds = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size)):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        enc = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
        "        input_ids = enc[\"input_ids\"].to(device)\n",
        "        attention_mask = enc[\"attention_mask\"].to(device)\n",
        "        outputs = bert(input_ids, attention_mask=attention_mask)\n",
        "        cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu()\n",
        "        all_embeds.append(cls_embeddings)\n",
        "    return torch.cat(all_embeds, dim=0).numpy()\n",
        "\n",
        "X = get_bert_embeddings(df_final[\"Question\"].tolist())\n",
        "y = df_final[label_cols].values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lr_KzT_mkxby",
        "outputId": "214dba6b-a47e-49c1-cd5f-058b6bec6dea"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [01:05<00:00, 65.43s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVqmUs3jqdxn",
        "outputId": "1498d5ae-9f54-4ca0-b056-3e42b595f1cb"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.21865879  0.22218935 -0.1446351  ... -0.4684483   0.5020686\n",
            "   1.0531839 ]\n",
            " [-0.59331685 -0.09805765 -0.28061807 ... -0.47908232  0.7137002\n",
            "   0.40427676]\n",
            " [-0.02363765  0.03771932  0.46016505 ... -0.5326069   0.45115116\n",
            "   0.51329595]\n",
            " ...\n",
            " [-0.40732935  0.09684791 -0.26155922 ... -0.53190845  0.60872567\n",
            "   0.67011017]\n",
            " [-0.15433094 -0.47385085  0.38733056 ... -0.57622606  0.31879175\n",
            "   0.17274837]\n",
            " [-0.37659708  0.04265192  0.11136115 ... -0.5242056   0.52409494\n",
            "   0.5302221 ]]\n",
            "[['0' '1' '0' '1' '0' '0']\n",
            " ['0' '0' '0' '1' '1' '0']\n",
            " ['0' '0' '0' '1' '1' '0']\n",
            " ...\n",
            " ['0' '0' '0' '0' '1' '0']\n",
            " ['1' '0' '0' '0' '0' '0']\n",
            " ['0' '0' '0' '1' '1' '0']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def hamming_score(y_true, y_pred):\n",
        "    return 1 - hamming_loss(y_true, y_pred)\n"
      ],
      "metadata": {
        "id": "YKl1BxfzsiZq"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = MultiOutputClassifier(LogisticRegression(class_weight='balanced', max_iter=500))\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "y = y.astype(int)\n",
        "\n",
        "# Example with Jaccard similarity (partial credit for overlaps)\n",
        "jaccard = make_scorer(jaccard_score, average='samples')\n",
        "scores = cross_val_score(clf, X, y, cv=kf, scoring=jaccard)\n",
        "print(\"5-fold Jaccard scores:\", scores)\n",
        "print(\"Mean Jaccard score:\", np.mean(scores))\n",
        "\n",
        "# Example with Hamming score\n",
        "hamming = make_scorer(hamming_score)\n",
        "scores = cross_val_score(clf, X, y, cv=kf, scoring=hamming)\n",
        "print(\"5-fold Hamming scores:\", scores)\n",
        "print(\"Mean Hamming score:\", np.mean(scores))\n",
        "\n",
        "# Example with Micro F1 (best for imbalanced multilabel)\n",
        "micro_f1 = make_scorer(f1_score, average='micro')\n",
        "scores = cross_val_score(clf, X, y, cv=kf, scoring=micro_f1)\n",
        "print(\"5-fold Micro F1 scores:\", scores)\n",
        "print(\"Mean Micro F1:\", np.mean(scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfIMsHmykxep",
        "outputId": "826823be-4513-4c68-c6f5-d82292740429"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5-fold Jaccard scores: [0.35851319 0.36211031 0.35326087 0.36413043 0.31702899]\n",
            "Mean Jaccard score: 0.3510087582108226\n",
            "5-fold Hamming scores: [0.74460432 0.7529976  0.74758454 0.75483092 0.73550725]\n",
            "Mean Hamming score: 0.7471049247558474\n",
            "5-fold Micro F1 scores: [0.42895442 0.44021739 0.43665768 0.42816901 0.40326975]\n",
            "Mean Micro F1: 0.42745365313808825\n"
          ]
        }
      ]
    }
  ]
}